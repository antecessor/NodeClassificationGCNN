{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test=load_data(path=\"./cora/\", dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GCNN import NodeClassificationGCNN\n",
    "\n",
    "model = NodeClassificationGCNN(features.shape[1], 256, np.max(labels.detach().numpy())+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out,label):\n",
    "    oneHotCodded = out.max(1)[1].type_as(label)\n",
    "    return oneHotCodded.eq(label).double().sum()/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ; accuracy: 0.85; loss: 1.035738229751587\n",
      "Validation epoch 0 ; accuracy: 0.7833333333333333; loss: 1.151475191116333\n",
      "Training epoch 1 ; accuracy: 0.8928571428571429; loss: 0.9011008143424988\n",
      "Validation epoch 1 ; accuracy: 0.82; loss: 1.0657345056533813\n",
      "Training epoch 2 ; accuracy: 0.9428571428571428; loss: 0.7846040725708008\n",
      "Validation epoch 2 ; accuracy: 0.84; loss: 0.9868790507316589\n",
      "Training epoch 3 ; accuracy: 0.9428571428571428; loss: 0.6801876425743103\n",
      "Validation epoch 3 ; accuracy: 0.8433333333333334; loss: 0.9153319001197815\n",
      "Training epoch 4 ; accuracy: 0.9428571428571428; loss: 0.5876632332801819\n",
      "Validation epoch 4 ; accuracy: 0.85; loss: 0.8517352342605591\n",
      "Training epoch 5 ; accuracy: 0.9642857142857143; loss: 0.49887800216674805\n",
      "Validation epoch 5 ; accuracy: 0.8433333333333334; loss: 0.7961025238037109\n",
      "Training epoch 6 ; accuracy: 0.9642857142857143; loss: 0.43331900238990784\n",
      "Validation epoch 6 ; accuracy: 0.83; loss: 0.7486196756362915\n",
      "Training epoch 7 ; accuracy: 0.9642857142857143; loss: 0.36543551087379456\n",
      "Validation epoch 7 ; accuracy: 0.82; loss: 0.7089692950248718\n",
      "Training epoch 8 ; accuracy: 0.9714285714285714; loss: 0.30906596779823303\n",
      "Validation epoch 8 ; accuracy: 0.8233333333333334; loss: 0.6767160296440125\n",
      "Training epoch 9 ; accuracy: 0.9714285714285714; loss: 0.26199135184288025\n",
      "Validation epoch 9 ; accuracy: 0.83; loss: 0.6510422825813293\n",
      "Training epoch 10 ; accuracy: 0.9785714285714285; loss: 0.22382886707782745\n",
      "Validation epoch 10 ; accuracy: 0.8266666666666667; loss: 0.6312863826751709\n",
      "Training epoch 11 ; accuracy: 0.9785714285714285; loss: 0.19622914493083954\n",
      "Validation epoch 11 ; accuracy: 0.83; loss: 0.6172115802764893\n",
      "Training epoch 12 ; accuracy: 0.9785714285714285; loss: 0.16488918662071228\n",
      "Validation epoch 12 ; accuracy: 0.83; loss: 0.6081998944282532\n",
      "Training epoch 13 ; accuracy: 0.9785714285714285; loss: 0.14119093120098114\n",
      "Validation epoch 13 ; accuracy: 0.8333333333333334; loss: 0.6033951640129089\n",
      "Training epoch 14 ; accuracy: 0.9857142857142858; loss: 0.11938749253749847\n",
      "Validation epoch 14 ; accuracy: 0.83; loss: 0.6023209095001221\n",
      "Training epoch 15 ; accuracy: 0.9857142857142858; loss: 0.10466938465833664\n",
      "Validation epoch 15 ; accuracy: 0.8333333333333334; loss: 0.6033796072006226\n",
      "Training epoch 16 ; accuracy: 0.9857142857142858; loss: 0.09318697452545166\n",
      "Validation epoch 16 ; accuracy: 0.8333333333333334; loss: 0.6069512963294983\n",
      "Training epoch 17 ; accuracy: 0.9857142857142858; loss: 0.07840447127819061\n",
      "Validation epoch 17 ; accuracy: 0.83; loss: 0.6111741065979004\n",
      "Training epoch 18 ; accuracy: 0.9857142857142858; loss: 0.06849560141563416\n",
      "Validation epoch 18 ; accuracy: 0.83; loss: 0.6158278584480286\n",
      "Training epoch 19 ; accuracy: 1.0; loss: 0.06051452457904816\n",
      "Validation epoch 19 ; accuracy: 0.8233333333333334; loss: 0.6207412481307983\n",
      "Training epoch 20 ; accuracy: 0.9928571428571429; loss: 0.056392211467027664\n",
      "Validation epoch 20 ; accuracy: 0.83; loss: 0.6257064938545227\n",
      "Training epoch 21 ; accuracy: 1.0; loss: 0.04737073928117752\n",
      "Validation epoch 21 ; accuracy: 0.83; loss: 0.6304909586906433\n",
      "Training epoch 22 ; accuracy: 1.0; loss: 0.04347199201583862\n",
      "Validation epoch 22 ; accuracy: 0.83; loss: 0.6350616216659546\n",
      "Training epoch 23 ; accuracy: 1.0; loss: 0.03635121136903763\n",
      "Validation epoch 23 ; accuracy: 0.8266666666666667; loss: 0.639846682548523\n",
      "Training epoch 24 ; accuracy: 1.0; loss: 0.03458769619464874\n",
      "Validation epoch 24 ; accuracy: 0.8266666666666667; loss: 0.6451320052146912\n",
      "Training epoch 25 ; accuracy: 1.0; loss: 0.03030184656381607\n",
      "Validation epoch 25 ; accuracy: 0.83; loss: 0.6506515741348267\n",
      "Training epoch 26 ; accuracy: 1.0; loss: 0.026676354929804802\n",
      "Validation epoch 26 ; accuracy: 0.83; loss: 0.6562418341636658\n",
      "Training epoch 27 ; accuracy: 1.0; loss: 0.025072647258639336\n",
      "Validation epoch 27 ; accuracy: 0.83; loss: 0.6628755927085876\n",
      "Training epoch 28 ; accuracy: 1.0; loss: 0.02096034400165081\n",
      "Validation epoch 28 ; accuracy: 0.83; loss: 0.6697357892990112\n",
      "Training epoch 29 ; accuracy: 1.0; loss: 0.019141174852848053\n",
      "Validation epoch 29 ; accuracy: 0.83; loss: 0.6763799786567688\n",
      "Training epoch 30 ; accuracy: 1.0; loss: 0.018518932163715363\n",
      "Validation epoch 30 ; accuracy: 0.8333333333333334; loss: 0.6819915771484375\n",
      "Training epoch 31 ; accuracy: 1.0; loss: 0.016953252255916595\n",
      "Validation epoch 31 ; accuracy: 0.8333333333333334; loss: 0.6875015497207642\n",
      "Training epoch 32 ; accuracy: 1.0; loss: 0.01620323210954666\n",
      "Validation epoch 32 ; accuracy: 0.8333333333333334; loss: 0.6926934123039246\n",
      "Training epoch 33 ; accuracy: 1.0; loss: 0.013660366646945477\n",
      "Validation epoch 33 ; accuracy: 0.83; loss: 0.6977360844612122\n",
      "Training epoch 34 ; accuracy: 1.0; loss: 0.012655305676162243\n",
      "Validation epoch 34 ; accuracy: 0.83; loss: 0.7024468183517456\n",
      "Training epoch 35 ; accuracy: 1.0; loss: 0.012577017769217491\n",
      "Validation epoch 35 ; accuracy: 0.8266666666666667; loss: 0.7069135308265686\n",
      "Training epoch 36 ; accuracy: 1.0; loss: 0.011094719171524048\n",
      "Validation epoch 36 ; accuracy: 0.8266666666666667; loss: 0.7114951610565186\n",
      "Training epoch 37 ; accuracy: 1.0; loss: 0.01042929757386446\n",
      "Validation epoch 37 ; accuracy: 0.8266666666666667; loss: 0.7158272862434387\n",
      "Training epoch 38 ; accuracy: 1.0; loss: 0.009463474154472351\n",
      "Validation epoch 38 ; accuracy: 0.8266666666666667; loss: 0.720733106136322\n",
      "Training epoch 39 ; accuracy: 1.0; loss: 0.009048203006386757\n",
      "Validation epoch 39 ; accuracy: 0.8266666666666667; loss: 0.7256141901016235\n",
      "Training epoch 40 ; accuracy: 1.0; loss: 0.00900658592581749\n",
      "Validation epoch 40 ; accuracy: 0.8266666666666667; loss: 0.730685830116272\n",
      "Training epoch 41 ; accuracy: 1.0; loss: 0.00786884780973196\n",
      "Validation epoch 41 ; accuracy: 0.8266666666666667; loss: 0.7358784675598145\n",
      "Training epoch 42 ; accuracy: 1.0; loss: 0.007324807345867157\n",
      "Validation epoch 42 ; accuracy: 0.8233333333333334; loss: 0.7408338785171509\n",
      "Training epoch 43 ; accuracy: 1.0; loss: 0.0070019932463765144\n",
      "Validation epoch 43 ; accuracy: 0.8233333333333334; loss: 0.7459306120872498\n",
      "Training epoch 44 ; accuracy: 1.0; loss: 0.006292921490967274\n",
      "Validation epoch 44 ; accuracy: 0.8233333333333334; loss: 0.7512419819831848\n",
      "Training epoch 45 ; accuracy: 1.0; loss: 0.006469361018389463\n",
      "Validation epoch 45 ; accuracy: 0.8233333333333334; loss: 0.7559353709220886\n",
      "Training epoch 46 ; accuracy: 1.0; loss: 0.006338594947010279\n",
      "Validation epoch 46 ; accuracy: 0.8233333333333334; loss: 0.7599595785140991\n",
      "Training epoch 47 ; accuracy: 1.0; loss: 0.0056102341040968895\n",
      "Validation epoch 47 ; accuracy: 0.8233333333333334; loss: 0.7635563015937805\n",
      "Training epoch 48 ; accuracy: 1.0; loss: 0.0063817729242146015\n",
      "Validation epoch 48 ; accuracy: 0.8233333333333334; loss: 0.7664517164230347\n",
      "Training epoch 49 ; accuracy: 1.0; loss: 0.0055374023504555225\n",
      "Validation epoch 49 ; accuracy: 0.8233333333333334; loss: 0.769396185874939\n",
      "Training epoch 50 ; accuracy: 1.0; loss: 0.0049768052995204926\n",
      "Validation epoch 50 ; accuracy: 0.8233333333333334; loss: 0.771897554397583\n",
      "Training epoch 51 ; accuracy: 1.0; loss: 0.004854944534599781\n",
      "Validation epoch 51 ; accuracy: 0.8233333333333334; loss: 0.7739130854606628\n",
      "Training epoch 52 ; accuracy: 1.0; loss: 0.00456951605156064\n",
      "Validation epoch 52 ; accuracy: 0.8233333333333334; loss: 0.7761765718460083\n",
      "Training epoch 53 ; accuracy: 1.0; loss: 0.00424202810972929\n",
      "Validation epoch 53 ; accuracy: 0.8233333333333334; loss: 0.7783976197242737\n",
      "Training epoch 54 ; accuracy: 1.0; loss: 0.004383750259876251\n",
      "Validation epoch 54 ; accuracy: 0.8233333333333334; loss: 0.7804734706878662\n",
      "Training epoch 55 ; accuracy: 1.0; loss: 0.004204821772873402\n",
      "Validation epoch 55 ; accuracy: 0.8233333333333334; loss: 0.7826143503189087\n",
      "Training epoch 56 ; accuracy: 1.0; loss: 0.0037967937532812357\n",
      "Validation epoch 56 ; accuracy: 0.8266666666666667; loss: 0.784911572933197\n",
      "Training epoch 57 ; accuracy: 1.0; loss: 0.0036733911838382483\n",
      "Validation epoch 57 ; accuracy: 0.8266666666666667; loss: 0.7871165871620178\n",
      "Training epoch 58 ; accuracy: 1.0; loss: 0.003602113574743271\n",
      "Validation epoch 58 ; accuracy: 0.8266666666666667; loss: 0.7893195152282715\n",
      "Training epoch 59 ; accuracy: 1.0; loss: 0.0034234148915857077\n",
      "Validation epoch 59 ; accuracy: 0.8266666666666667; loss: 0.7915013432502747\n",
      "Training epoch 60 ; accuracy: 1.0; loss: 0.003638585563749075\n",
      "Validation epoch 60 ; accuracy: 0.8266666666666667; loss: 0.7933529615402222\n",
      "Training epoch 61 ; accuracy: 1.0; loss: 0.0033050342462956905\n",
      "Validation epoch 61 ; accuracy: 0.8266666666666667; loss: 0.7952370643615723\n",
      "Training epoch 62 ; accuracy: 1.0; loss: 0.003417589236050844\n",
      "Validation epoch 62 ; accuracy: 0.8266666666666667; loss: 0.7970391511917114\n",
      "Training epoch 63 ; accuracy: 1.0; loss: 0.0031595989130437374\n",
      "Validation epoch 63 ; accuracy: 0.8266666666666667; loss: 0.7989793419837952\n",
      "Training epoch 64 ; accuracy: 1.0; loss: 0.0033502914011478424\n",
      "Validation epoch 64 ; accuracy: 0.8233333333333334; loss: 0.8005803823471069\n",
      "Training epoch 65 ; accuracy: 1.0; loss: 0.002987651852890849\n",
      "Validation epoch 65 ; accuracy: 0.8233333333333334; loss: 0.8022570610046387\n",
      "Training epoch 66 ; accuracy: 1.0; loss: 0.0029829638078808784\n",
      "Validation epoch 66 ; accuracy: 0.8233333333333334; loss: 0.8041138648986816\n",
      "Training epoch 67 ; accuracy: 1.0; loss: 0.002805104712024331\n",
      "Validation epoch 67 ; accuracy: 0.8233333333333334; loss: 0.8059544563293457\n",
      "Training epoch 68 ; accuracy: 1.0; loss: 0.0028078267350792885\n",
      "Validation epoch 68 ; accuracy: 0.8233333333333334; loss: 0.8077366948127747\n",
      "Training epoch 69 ; accuracy: 1.0; loss: 0.002697701333090663\n",
      "Validation epoch 69 ; accuracy: 0.8233333333333334; loss: 0.8094660639762878\n",
      "Training epoch 70 ; accuracy: 1.0; loss: 0.002846658928319812\n",
      "Validation epoch 70 ; accuracy: 0.8233333333333334; loss: 0.8110876083374023\n",
      "Training epoch 71 ; accuracy: 1.0; loss: 0.0024636622983962297\n",
      "Validation epoch 71 ; accuracy: 0.8233333333333334; loss: 0.8126333951950073\n",
      "Training epoch 72 ; accuracy: 1.0; loss: 0.0025944949593394995\n",
      "Validation epoch 72 ; accuracy: 0.8233333333333334; loss: 0.8141143918037415\n",
      "Training epoch 73 ; accuracy: 1.0; loss: 0.0024421759881079197\n",
      "Validation epoch 73 ; accuracy: 0.8233333333333334; loss: 0.8156850934028625\n",
      "Training epoch 74 ; accuracy: 1.0; loss: 0.0028194391634315252\n",
      "Validation epoch 74 ; accuracy: 0.8233333333333334; loss: 0.8173706531524658\n",
      "Training epoch 75 ; accuracy: 1.0; loss: 0.002596545498818159\n",
      "Validation epoch 75 ; accuracy: 0.8233333333333334; loss: 0.8191975355148315\n",
      "Training epoch 76 ; accuracy: 1.0; loss: 0.0024829779285937548\n",
      "Validation epoch 76 ; accuracy: 0.82; loss: 0.8209146857261658\n",
      "Training epoch 77 ; accuracy: 1.0; loss: 0.0024311156012117863\n",
      "Validation epoch 77 ; accuracy: 0.82; loss: 0.822634756565094\n",
      "Training epoch 78 ; accuracy: 1.0; loss: 0.002420630306005478\n",
      "Validation epoch 78 ; accuracy: 0.82; loss: 0.8241687417030334\n",
      "Training epoch 79 ; accuracy: 1.0; loss: 0.0024147971998900175\n",
      "Validation epoch 79 ; accuracy: 0.82; loss: 0.8256660103797913\n",
      "Training epoch 80 ; accuracy: 1.0; loss: 0.002225078409537673\n",
      "Validation epoch 80 ; accuracy: 0.82; loss: 0.826932430267334\n",
      "Training epoch 81 ; accuracy: 1.0; loss: 0.0021523621398955584\n",
      "Validation epoch 81 ; accuracy: 0.8233333333333334; loss: 0.8280893564224243\n",
      "Training epoch 82 ; accuracy: 1.0; loss: 0.0021574427373707294\n",
      "Validation epoch 82 ; accuracy: 0.8233333333333334; loss: 0.8293233513832092\n",
      "Training epoch 83 ; accuracy: 1.0; loss: 0.002176598645746708\n",
      "Validation epoch 83 ; accuracy: 0.8233333333333334; loss: 0.8304699063301086\n",
      "Training epoch 84 ; accuracy: 1.0; loss: 0.0020045614801347256\n",
      "Validation epoch 84 ; accuracy: 0.8233333333333334; loss: 0.8316888213157654\n",
      "Training epoch 85 ; accuracy: 1.0; loss: 0.0021404067520052195\n",
      "Validation epoch 85 ; accuracy: 0.8233333333333334; loss: 0.8326596021652222\n",
      "Training epoch 86 ; accuracy: 1.0; loss: 0.0022307911422103643\n",
      "Validation epoch 86 ; accuracy: 0.8233333333333334; loss: 0.8336148858070374\n",
      "Training epoch 87 ; accuracy: 1.0; loss: 0.0021963969338685274\n",
      "Validation epoch 87 ; accuracy: 0.8233333333333334; loss: 0.8347473740577698\n",
      "Training epoch 88 ; accuracy: 1.0; loss: 0.0019876686856150627\n",
      "Validation epoch 88 ; accuracy: 0.8233333333333334; loss: 0.8353592753410339\n",
      "Training epoch 89 ; accuracy: 1.0; loss: 0.001848108833655715\n",
      "Validation epoch 89 ; accuracy: 0.8233333333333334; loss: 0.8360602855682373\n",
      "Training epoch 90 ; accuracy: 1.0; loss: 0.0018176953308284283\n",
      "Validation epoch 90 ; accuracy: 0.8233333333333334; loss: 0.8368771076202393\n",
      "Training epoch 91 ; accuracy: 1.0; loss: 0.0018968558870255947\n",
      "Validation epoch 91 ; accuracy: 0.8233333333333334; loss: 0.8377967476844788\n",
      "Training epoch 92 ; accuracy: 1.0; loss: 0.0019649674650281668\n",
      "Validation epoch 92 ; accuracy: 0.8233333333333334; loss: 0.8388368487358093\n",
      "Training epoch 93 ; accuracy: 1.0; loss: 0.0018390720942988992\n",
      "Validation epoch 93 ; accuracy: 0.8233333333333334; loss: 0.8398113250732422\n",
      "Training epoch 94 ; accuracy: 1.0; loss: 0.0019632542971521616\n",
      "Validation epoch 94 ; accuracy: 0.8233333333333334; loss: 0.840717077255249\n",
      "Training epoch 95 ; accuracy: 1.0; loss: 0.0017019917722791433\n",
      "Validation epoch 95 ; accuracy: 0.8233333333333334; loss: 0.841696560382843\n",
      "Training epoch 96 ; accuracy: 1.0; loss: 0.0018441756255924702\n",
      "Validation epoch 96 ; accuracy: 0.8233333333333334; loss: 0.8425909876823425\n",
      "Training epoch 97 ; accuracy: 1.0; loss: 0.0017413388704881072\n",
      "Validation epoch 97 ; accuracy: 0.82; loss: 0.8435106873512268\n",
      "Training epoch 98 ; accuracy: 1.0; loss: 0.0018813913920894265\n",
      "Validation epoch 98 ; accuracy: 0.82; loss: 0.8445113897323608\n",
      "Training epoch 99 ; accuracy: 1.0; loss: 0.0019401954486966133\n",
      "Validation epoch 99 ; accuracy: 0.82; loss: 0.845614492893219\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs=100\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_labels=labels[idx_train]\n",
    "    val_labels=labels[idx_val]\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss=F.nll_loss(output[idx_train],train_labels)\n",
    "    print(f\"Training epoch {epoch} ; accuracy: {accuracy(output[idx_train],train_labels)}; loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss=F.nll_loss(output[idx_val],val_labels)\n",
    "    print(f\"Validation epoch {epoch} ; accuracy: {accuracy(output[idx_val],val_labels)}; loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_labels=labels[idx_test]\n",
    "output = model(features, adj)\n",
    "loss=F.nll_loss(output[idx_test],test_labels)\n",
    "print(f\"Test set ; accuracy: {accuracy(output[idx_test],test_labels)}; loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "366252978e52bb2df929d3934aeb3ff29dfa67e45e575a59a0b0194f7beef5a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
